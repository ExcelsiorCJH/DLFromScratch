{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chap05 - 오차역전파법 Backpropagation\n",
    "\n",
    "> [Chap04 - 신경망 학습](https://github.com/ExcelsiorCJH/DLFromScratch/blob/master/Chap04-Neural_Network_Traing/Chap04-Neural_Network_Training.ipynb)에서는 가중치 매개변수의 기울기를 미분을 이용해 구했다. 이러한 방법은 간단하지만 시간이 오래 걸리는 단점이 있다. \n",
    "이번 장에서는 가중치 매개변수의 기울기를 효율적으로 계산하는 **오차역전파법(backpropagation)**에 대해 알아보도록 하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 계산 그래프\n",
    "\n",
    "**계산 그래프(computational graph)**는 계산 과정을 그래프로 나타낸 것이며, **노드**(node)와 **엣지**(edge)로 표현된다. 노드는 연산을 정의하며, 엣지는 데이터가 흘러가는 방향을 나타낸다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1 계산 그래프로 풀다\n",
    "\n",
    "> 현빈 군은 슈퍼에서 사과를 2개, 귤을 3개 샀습니다. 사과는 1개에 100원, 귤은 1개 150원입니다. 소비세가 10%일 때 지불 금액을 구하라.\n",
    "\n",
    "<img src=\"./images/5-3.png\" width=\"75%\" height=\"75%\"/>\n",
    "\n",
    "1. 계산 그래프를 구성한다.\n",
    "2. 그래프에서 계산을 왼쪽에서 오른쪽으로 진행한다. → **순전파**(forward propagation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2 국소적 계산\n",
    "\n",
    "계산 그래프의 특징은 '국소적 계산'을 통해 최종 결과를 얻는 것이다. 즉, '자신과 직접 관계된' 범위 내에서만 계산이 이루어 진다. \n",
    "\n",
    "<img src=\"./images/5-4.png\" width=\"75%\" height=\"75%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.3 왜 계산 그래프로 푸는가?\n",
    "\n",
    "계산그래프의 장점은 다음과 같다.\n",
    "\n",
    "- **국소적 계산**을 통해 각 노드의 계산에 집중하여 문제를 단순화할 수 있다.\n",
    "- **역전파**를 통해 '미분'을 효율적으로 계산할 수 있다.\n",
    "\n",
    "'사과 가격이 오르면 최종 금액에 어떠한 영향을 주는가'에 대해서 **사과 가격에 대한 지불 금액의 미분**을 구해 계산할 수 있다. 사과의 값을 $x$, 지불 금액을 $L$라 했을 때, $\\frac{\\partial L}{\\partial x}$를 구하는 것이다. 이러한 미분 값은 사과 값($x$)가 '아주 조금'올랐을 때 지불 금액($L$)이 얼마나 증가하는지를 나타낸다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/5-5.PNG\" width=\"75%\" height=\"75%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 연쇄 법칙 - Chain Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1 계산 그래프의 역전파\n",
    "\n",
    "먼저, 역전파 계산 예제로 $y=f(x)$ 의 역전파를 계산해보자. \n",
    "\n",
    "<img src=\"./images/5-6.PNG\" width=\"50%\" height=\"50%\"/>\n",
    "\n",
    "위의 그림에서 처럼 역전파 계산 순서는 신호 $E$에 노드($f$)의 국소적 미분 $\\left( \\frac{\\partial y}{\\partial x} \\right)$을 곱한 후 엣지(edge)를 통해 다음 노드로 전달하는 것이다. 여기서 국소적 미분은 순전파 때의 $y = f(x)$에 대한 미분을 구하는 것이고, 이것은 $x$에 대한 $y$의 미분 $\\left( \\frac{\\partial y}{\\partial x} \\right)$을 구한다는 의미이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2 연쇄법칙이란?\n",
    "\n",
    "> *합성함수의 미분은 합성 함수를 구성하는 각 함수의 미분의 곱으로 나타낼 수 있다.*\n",
    "\n",
    "$$\n",
    "t = x + y\n",
    "$$\n",
    "\n",
    "$$\n",
    "z = t^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial t} \\frac{\\partial t}{\\partial x}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial t} = 2t\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial t}{\\partial x} = 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial t} \\frac{\\partial t}{\\partial x} = 2t \\cdot 1 = 2(x + y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.3 연쇄법칙과 계산 그래프\n",
    "\n",
    "5.2.2의 예제 식을 계산 그래프로 나타내면 다음과 같다.\n",
    "\n",
    "<img src=\"./images/5-8.png\" width=\"90%\" height=\"90%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 역전파"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1 덧셈 노드의 역전파\n",
    "\n",
    "$$\n",
    "z = x + y\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial x} = 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial y} = 1\n",
    "$$\n",
    "\n",
    "덧셈 노드의 역전파는 입력값을 그대로 흘려보낸다.\n",
    "\n",
    "<img src=\"./images/5-9.PNG\" width=\"80%\" height=\"80%\"/>\n",
    "\n",
    "위의 $z=x+y$계산은 큰 계산 그래프의 중간 어딘가에 존재한다고 가정했기 때문에, 이 계산 그래프의 앞부분(상류)에서부터 $\\frac{\\partial L}{\\partial z}$가 전해졌다고 가정한다.\n",
    "\n",
    "<img src=\"./images/5-10.png\" width=\"70%\" height=\"70%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.2 곱셈 노드의 역전파\n",
    "\n",
    "$$\n",
    "z =xy\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial x} = y \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial y} = x\n",
    "$$\n",
    "\n",
    "<img src=\"./images/5-12.PNG\" width=\"80%\" height=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 단순한 계층 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.1 곱셈 계층\n",
    "\n",
    "`forward()`는 순전파, `backward()`는 역전파이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x * y\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.y  # x와 y를 바꾼다.\n",
    "        dy = dout * self.x\n",
    "        \n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/5-16.PNG\" width=\"75%\" height=\"75%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220\n",
      "2.2, 110, 200\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "tax = 1.1\n",
    "\n",
    "# 계층들\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# 순전파\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "price = mul_tax_layer.forward(apple_price, tax)\n",
    "\n",
    "print('%d' % price)\n",
    "\n",
    "# 역전파\n",
    "dprice = 1\n",
    "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print(\"%.1f, %d, %d\" % (dapple, dapple_num, dtax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.2 덧셈 계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        out = x + y\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/5-17.PNG\" width=\"75%\" height=\"75%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715\n",
      "110, 2.2, 3.3, 165, 650\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "orange = 150\n",
    "orange_num = 3\n",
    "tax = 1.1\n",
    "\n",
    "# 계층들\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_orange_layer = MulLayer()\n",
    "add_apple_orange_layer = AddLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# 순전파\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "orange_price = mul_orange_layer.forward(orange, orange_num)\n",
    "all_price = add_apple_orange_layer.forward(apple_price, orange_price)\n",
    "price = mul_tax_layer.forward(all_price, tax)\n",
    "\n",
    "# 역전파\n",
    "dprice = 1\n",
    "dall_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)\n",
    "dorange, dorange_num = mul_orange_layer.backward(dorange_price)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print('%d' % price)\n",
    "print(\"%d, %.1f, %.1f, %d, %d\" % (dapple_num, dapple, dorange, dorange_num, dtax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 활성화 함수 계층 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.1 ReLU 계층\n",
    "\n",
    "ReLU의 식은 다음과 같다.\n",
    "\n",
    "$$\n",
    "y=\\begin{cases} x\\quad (x>0) \\\\ 0\\quad (x\\le 0) \\end{cases}\n",
    "$$\n",
    "\n",
    "위의 식에서 $x$에 대한 $y$의 미분은 아래와 같다.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x}=\\begin{cases} 1\\quad (x>0) \\\\ 0\\quad (x\\le 0) \\end{cases}\n",
    "$$\n",
    "\n",
    "<img src=\"./images/5-18.PNG\" width=\"75%\" height=\"75%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common/layers.py\n",
    "\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 `Relu` 클래스에서 `mask` 인스턴스 변수는 아래와 같이 `True/False`로 구성되는 NumPy 배열이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      " [[ 1.  -0.5]\n",
      " [-2.   3. ]]\n",
      "mask:\n",
      " [[False  True]\n",
      " [ True False]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1., -.5], \n",
    "              [-2., 3.]])\n",
    "print('x:\\n', x)\n",
    "\n",
    "mask = (x <= 0)\n",
    "print('mask:\\n', mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.2 Sigmoid 계층\n",
    "\n",
    "$$\n",
    "y = \\frac{1}{1 + \\text{exp(-x)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/5-22.PNG\" width=\"75%\" height=\"75%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{eqnarray} \\frac { \\partial L }{ \\partial y } y^{ 2 }{ exp }(-x) & = & \\frac { \\partial L }{ \\partial y } \\frac { 1 }{ \\left( 1+{ exp }(-x) \\right) ^{ 2 } } \\text{exp}(-x)  \\\\  & = & \\frac { \\partial L }{ \\partial y } \\frac { 1 }{ 1+{ exp }(-x) } \\frac { { exp(-x) } }{ 1+{ exp(-x) } }  \\\\  & = & \\frac { \\partial L }{ \\partial y } y(1-y) \\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common/layers\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Affine/Softmax 계층 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.1 Affine 계층\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{X}} = \\frac{\\partial L}{\\partial \\mathbf{Y}} \\cdot \\mathbf{W}^T\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{X}} = \\mathbf{X}^T \\cdot \\frac{\\partial L}{\\partial \\mathbf{Y}} \n",
    "$$\n",
    "\n",
    "<img src=\"./images/5-25.PNG\" width=\"75%\" height=\"75%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.2 배치용 Affine 계층\n",
    "\n",
    "<img src=\"./images/5-27.PNG\" width=\"75%\" height=\"75%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common/layers.py\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.3 Softmax-with-Loss 계층\n",
    "\n",
    "> 딥러닝에서는 **학습**과 **추론** 두 가지가 있다. 일반적으로 추론일 때는 **Softmax** 계층(layer)을 사용하지 않는다. Softmax 계층 앞의 Affine 계층의 출력을 **점수**(score)라고 하는데, 딥러닝의 추론에서는 답을 하나만 예측하는 경우에는 가장 높은 점수만 알면 되므로 Softmax 계층이 필요없다. 반면, 딥러닝을 **학습**할 때는 Softmax 계층이 필요하다.\n",
    "\n",
    "<img src=\"./images/5-28.png\" width=\"75%\" height=\"75%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "소프트맥스(softmax) 계층을 구현할때, 손실함수인 교차 엔트로피 오차(cross entropy error)도 포함하여 아래의 그림과 같이 **Softmax-with-Loss** 계층을 구현한다.\n",
    "\n",
    "<img src=\"./images/5-29.png\" width=\"85%\" height=\"85%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common/layers.py\n",
    "import os, sys\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None  # 손실\n",
    "        self.y = None  # softmax의 출력\n",
    "        self.t = None  # 정답 레이블(one-hot)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 오차역전파법 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7.1 신경망 학습의 전체 그림\n",
    "\n",
    "> 딥러닝은 손실함수의 값이 최소로 되도록 가중치와 편향인 매개변수를 조정하는 과정을 **학습**이라고 한다. 딥러닝 학습은 다음 4단계와 같다.\n",
    "1. **미니배치(mini-batch)** : Train 데이터 중 랜덤하게 샘플을 추출하는데 이것을 미니배치라고 하며, 미니배치의 손실함수 값을 줄이는 것이 학습의 목표이다. \n",
    "2. **기울기 계산** : 손실함수의 값을 작게하는 방향을 가리키는 가중치($\\mathbf{W}$) 매개변수의 기울기를 구한다.\n",
    "3. **매개변수 갱신** : 가중치 매개변수를 기울기 방향으로 학습률(learning rate)만큼 갱신한다.\n",
    "4. **반복** 1~3 단계를 반복한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7.2 오차역전파법을 적용한 신경망 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two_layer_net.py\n",
    "import sys,os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "    '''2층 신경망 구현'''\n",
    "    def __init__(self, input_size, \n",
    "                 hidden_size, output_size, weight_init_std=0.01):\n",
    "        '''\n",
    "        초기화 수행\n",
    "        Params:\n",
    "            - input_size: 입력층 뉴런 수\n",
    "            - hidden_size: 은닉층 뉴런 수\n",
    "            - output_size: 출력층 뉴런 수\n",
    "            - weight_init_std: 가중치 초기화 시 정규분포의 스케일\n",
    "        '''\n",
    "        # 가중치 초기화\n",
    "        self.params = {\n",
    "            'W1': weight_init_std * np.random.randn(input_size, hidden_size),\n",
    "            'b1': np.zeros(hidden_size),\n",
    "            'W2': weight_init_std * np.random.randn(hidden_size, output_size),\n",
    "            'b2': np.zeros(output_size)\n",
    "        }\n",
    "        \n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict({\n",
    "            'Affine1': Affine(self.params['W1'], self.params['b1']),\n",
    "            'Relu1': Relu(),\n",
    "            'Affine2': Affine(self.params['W2'], self.params['b2'])\n",
    "        })\n",
    "        \n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "        \n",
    "    \n",
    "    def predict(self, x):\n",
    "        '''예측(추론)\n",
    "            Pararms:\n",
    "                - x: 이미지 데이터'''\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        '''\n",
    "        손실함수의 값을 계산\n",
    "        Params:\n",
    "            - x: 이미지데이터, t: 정답 레이블\n",
    "        '''\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        '''\n",
    "        정확도 계산\n",
    "        Params:\n",
    "            - x: 이미지 데이터\n",
    "            - t: 정답 레이블\n",
    "        '''\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1:\n",
    "            t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y==t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        '''\n",
    "        미분을 통한 가중치 매개변수의 기울기 계산\n",
    "        Params:\n",
    "            - x: 이미지 데이터\n",
    "            - t: 정답 레이블 \n",
    "        '''\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {\n",
    "            'W1': numerical_gradient(loss_W, self.params['W1']),\n",
    "            'b1': numerical_gradient(loss_W, self.params['b1']),\n",
    "            'W2': numerical_gradient(loss_W, self.params['W2']),\n",
    "            'b2': numerical_gradient(loss_W, self.params['b2'])\n",
    "        }\n",
    "        return grads\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "        \n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "            \n",
    "        # 결과 저장\n",
    "        grads = {\n",
    "            'W1': self.layers['Affine1'].dW, 'b1': self.layers['Affine1'].db,\n",
    "            'W2': self.layers['Affine2'].dW, 'b2': self.layers['Affine2'].db\n",
    "        }\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7.3 오차역전파법으로 구한 기울기 검증하기\n",
    "\n",
    "수치 미분을 통해 구한 기울기와 오차역전파법의 결과를 비교하여 오차역전파를 제대로 구현했는지 검증하는 작업을 **기울기 확인(gradient check)**이라고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 : 4.479721446541244e-10\n",
      "b1 : 2.5485543061868916e-09\n",
      "W2 : 4.349602602871501e-09\n",
      "b2 : 1.393278526204411e-07\n",
      "Wall time: 6.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# gradient_check.py\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "# mnist load\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=28*28, hidden_size=50, output_size=10)\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "\n",
    "# 각 가중치의 절대 오차의 평균을 구한다.\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average(np.abs(grad_backprop[key] - grad_numerical[key]))\n",
    "    print(key,\":\", str(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7.4 오차역전파법을 사용한 학습 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  600\tTrain acc: 0.90450\tTest acc: 0.90560\n",
      "Step: 1200\tTrain acc: 0.92288\tTest acc: 0.92570\n",
      "Step: 1800\tTrain acc: 0.93220\tTest acc: 0.93200\n",
      "Step: 2400\tTrain acc: 0.94605\tTest acc: 0.94460\n",
      "Step: 3000\tTrain acc: 0.95430\tTest acc: 0.95210\n",
      "Step: 3600\tTrain acc: 0.95993\tTest acc: 0.95870\n",
      "Step: 4200\tTrain acc: 0.96360\tTest acc: 0.95870\n",
      "Step: 4800\tTrain acc: 0.96682\tTest acc: 0.96320\n",
      "Step: 5400\tTrain acc: 0.96930\tTest acc: 0.96380\n",
      "Step: 6000\tTrain acc: 0.97108\tTest acc: 0.96450\n",
      "Step: 6600\tTrain acc: 0.97318\tTest acc: 0.96690\n",
      "Step: 7200\tTrain acc: 0.97557\tTest acc: 0.96890\n",
      "Step: 7800\tTrain acc: 0.97698\tTest acc: 0.96760\n",
      "Step: 8400\tTrain acc: 0.97808\tTest acc: 0.96990\n",
      "Step: 9000\tTrain acc: 0.97835\tTest acc: 0.97030\n",
      "Step: 9600\tTrain acc: 0.98035\tTest acc: 0.97020\n",
      "Optimization finished!\n",
      "Wall time: 26.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train_neuralnet.py\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "# mnist load\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=28*28, hidden_size=50, output_size=10)\n",
    "\n",
    "# Train Parameters\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "train_loss_list, train_acc_list, test_acc_list = [], [], []\n",
    "\n",
    "for step in range(1, iters_num+1):\n",
    "    # get mini-batch\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch) # 수치 미분 방식\n",
    "    grad = network.gradient(x_batch, t_batch) # 오차역전파법 방식(압도적으로 빠르다)\n",
    "    \n",
    "    # Update\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    # loss\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if step % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print('Step: {:4d}\\tTrain acc: {:.5f}\\tTest acc: {:.5f}'.format(step, \n",
    "                                                                        train_acc,\n",
    "                                                                        test_acc))\n",
    "        \n",
    "print('Optimization finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.8 정리\n",
    "\n",
    "- 계산 그래프를 이용하면 계산 과정을 시각적으로 파악할 수 있다.\n",
    "- 계산 그래프의 노드는 국소적 계산으로 구성된다. 국소적 계산을 조합해 전체 계산을 구성한다.\n",
    "- 계산 그래프의 순전파는 통상의 계산을 수행한다. 한편, 계산 그래프의 역전파로는 각 노드의 미분을 구할\n",
    "수 있다.\n",
    "- 신경망의 구성 요소를 계층으로 구현하여 기울기를 효율적으로 계산할 수 있다(오차역전파법).\n",
    "- 수치 미분과 오차역전파법의 결과를 비교하면 오차역전파법의 구현에 잘못이 없는지 확인할 수 있다(기울\n",
    "기 확인)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "study"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
